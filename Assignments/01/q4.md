# Question 4:
## __An agent that senses only partial information about the state cannot be perfectly rational.__
False, Perfect rationality would mean selecting the best course of action based on available information, even with partial observability, an agent can make optimal decisions given its knowledge bank. A counterexample could be a self-driving car that does not have complete visibility in foggy weather conditions, yet it can still act perfectly rational by using its sensor fusion (LIDAR, radar, & cameras) to estimate road conditions, as well as predicting the movement of vehicles based on past observations and adapting speed and braking distances using probabilistic models. 
## __There exist task environments in which no pure reflex agent can behave rationally.__
True, as pure reflex agents are programmed to work in simple environments as they react based on current percepts and do not store history or perform any long-term planning. A chess-playing agent that only considers the current state of the board can’t behave rationally because optimal play requires strategic depth which comes from storing & analyzing historical chess-playing data.
## __There exists a task environment in which every agent is rational.__
False, as rationality depends on how an agent would choose its action relative to its goal, some agents might make less optimal decisions, rendering them irrational in that certain task environment. Like, in a complex maze navigation task, an agent that moves randomly can’t be rational whilst a planned search agent would prove to be rational in that instance. 
## __The input to an agent program is the same as the input to the agent function.__
False, the Agent Function is an abstract mapping of the percept sequence to actions whilst the Agent Program is the specific implementation of that specific function. In a vacuum cleaner agent, the agent function defines the mapping from percept histories to action while the agent program implements this function with a set of certain specific algorithms.
## __Every agent function is implementable by some program/machine combination.__
False, the space of possible agent functions is vast, but not all of them can be computed efficiently, as some require an infinite memory or computational time/power. An example can be taken of the agent function solving the Halting problem, which cannot be implemented as Turing proved it to be undecidable. 
## __Suppose an agent selects its action uniformly at random from the set of possible actions. There exists a deterministic task environment in which this agent is rational.__
True, an agent acting uniformly can be rational if the expected utility from it is the same for all of the actions in a certain deterministic environment, like in a lottery selection program, if all choices have an equal probability then choosing randomly is the best strategy. 
## __It is possible for a given agent to be perfectly rational in two distinct task environments.__
True, if an agent can follow an optimal decision policy that works across multiple environments then it can remain rational in both. Like, a thermostat agent that is programmed to maintain room temperature can be rational in both a small house and a large office building, if it can adapt to different spaces adequately.
